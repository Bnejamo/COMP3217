{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7ee74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benjaminlewis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/benjaminlewis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/benjaminlewis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/benjaminlewis/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c72822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scary shit #hurricane #NY http\n",
      "Scary shit hurricane NY http\n",
      "Scary shit hurricane NY http\n",
      "Scary shit #hurricane #NY \n",
      "http\n"
     ]
    }
   ],
   "source": [
    "tweet='Scary shit #hurricane #NY http://t.co/e4JLBUfH'\n",
    "tweet = re.sub(r'http\\S+', 'http', tweet)\n",
    "print(tweet)\n",
    "tweet = re.sub(r'[^a-zA-Z0-9]+', ' ', tweet)\n",
    "print(tweet)\n",
    "tweet = re.sub(r'@\\S+', '', tweet)\n",
    "print(tweet)\n",
    "\n",
    "tweet='Scary shit #hurricane #NY http://t.co/e4JLBUfH'\n",
    "tweet = re.sub(r'http\\S+', '', tweet)\n",
    "print(tweet)\n",
    "\n",
    "\n",
    "tweet = re.sub(r'http\\s+', '', 'http')\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec07805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "trainData = pandas.read_csv(\"mediaeval-2015-trainingset.txt\", sep=\"\t\")\n",
    "testData = pandas.read_csv(\"mediaeval-2015-testset.txt\", sep=\"\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec8c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pandas.DataFrame(data = trainData)\n",
    "df_test = pandas.DataFrame(data = testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4555db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: label, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.label.where(df_train.label =='1').dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19a979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e2f07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[(df_train.label == 'humor'),'label'] = 'fake'\n",
    "df_test.loc[(df_test.label == 'humor'),'label'] = 'fake'\n",
    "df_train=df_train.drop_duplicates(subset=['tweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e6cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "        # Remove special characters, URLs, and mentions\n",
    "        tweet = re.sub(r'http\\S+', 'http', tweet)\n",
    "        tweet = re.sub(r'[^a-zA-Z0-9]+', ' ', tweet)\n",
    "        tweet= tweet.lower()\n",
    "\n",
    "        \n",
    "        # Remove stop words\n",
    "        stopwords_set = set(stopwords.words())\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stopwords_set])\n",
    "\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = nltk.word_tokenize(tweet)\n",
    "        tweet = ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
    "        \n",
    "        return tweet\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92e014ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcF1(y_pred,y_test):\n",
    "\n",
    "    truePositives = sum((y_pred == 'fake') & (y_test == 'fake'))\n",
    "\n",
    "\n",
    "\n",
    "    positives = sum(y_pred == 'fake')\n",
    "\n",
    "\n",
    "    precision = truePositives / positives\n",
    "\n",
    "    totalPositives = sum(y_test == 'fake')\n",
    "\n",
    "\n",
    "    recall = truePositives / totalPositives\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "487e9a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pTweets']= df_train['tweetText'].apply(lambda text: process_tweet(text))\n",
    "df_test['pTweets']= df_test['tweetText'].apply(lambda text: process_tweet(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1dcb29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>pTweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¿Se acuerdan de la película: “El día después d...</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>acuerdan cula despu ana recuerda pasando hurac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda i...</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>milenagimon miren sandy ny tremenda imagen hur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>buena foto hurac sandy recuerda cula independe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>scary shit hurricane ny http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>fave place world nyc hurricane sandy statueofl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>263364439582060545</td>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane htt...</td>\n",
       "      <td>163674788</td>\n",
       "      <td>sandyA_fake_23</td>\n",
       "      <td>classycg</td>\n",
       "      <td>Tue Oct 30 19:39:10 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>42nd time square nyc subway hurricane http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>262927032705490944</td>\n",
       "      <td>Just in time for #halloween a photo of #hurric...</td>\n",
       "      <td>246153081</td>\n",
       "      <td>sandyA_fake_14</td>\n",
       "      <td>j_unit87</td>\n",
       "      <td>Mon Oct 29 14:41:04 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>time halloween photo hurricane sandy frankenst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>263321078884077568</td>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out ...</td>\n",
       "      <td>199565482</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>MrBlakMagik</td>\n",
       "      <td>Tue Oct 30 16:46:52 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>crazy hurricane sandy prayer family friend eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>263111677485142017</td>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #U...</td>\n",
       "      <td>78475739</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>safi37</td>\n",
       "      <td>Tue Oct 30 02:54:46 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>sandy newyork hurricane statueofliberty usa http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>262977091983785985</td>\n",
       "      <td>#nyc #hurricane http://t.co/Gv3QxZlq</td>\n",
       "      <td>869777653</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>kingmichael03</td>\n",
       "      <td>Mon Oct 29 17:59:59 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>nyc hurricane http</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId                                          tweetText  \\\n",
       "0  263046056240115712  ¿Se acuerdan de la película: “El día después d...   \n",
       "1  262995061304852481  @milenagimon: Miren a Sandy en NY!  Tremenda i...   \n",
       "2  262979898002534400  Buena la foto del Huracán Sandy, me recuerda a...   \n",
       "3  262996108400271360     Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4  263018881839411200  My fave place in the world #nyc #hurricane #sa...   \n",
       "5  263364439582060545  42nd #time #square #NYC #subway #hurricane htt...   \n",
       "6  262927032705490944  Just in time for #halloween a photo of #hurric...   \n",
       "7  263321078884077568  Crazy pic of #Hurricane #Sandy prayers go out ...   \n",
       "8  263111677485142017  #sandy #newyork #hurricane #statueofliberty #U...   \n",
       "9  262977091983785985               #nyc #hurricane http://t.co/Gv3QxZlq   \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "5  163674788  sandyA_fake_23        classycg  Tue Oct 30 19:39:10 +0000 2012   \n",
       "6  246153081  sandyA_fake_14        j_unit87  Mon Oct 29 14:41:04 +0000 2012   \n",
       "7  199565482  sandyA_fake_29     MrBlakMagik  Tue Oct 30 16:46:52 +0000 2012   \n",
       "8   78475739  sandyA_fake_15          safi37  Tue Oct 30 02:54:46 +0000 2012   \n",
       "9  869777653  sandyA_fake_29   kingmichael03  Mon Oct 29 17:59:59 +0000 2012   \n",
       "\n",
       "  label                                            pTweets  \n",
       "0  fake  acuerdan cula despu ana recuerda pasando hurac...  \n",
       "1  fake  milenagimon miren sandy ny tremenda imagen hur...  \n",
       "2  fake  buena foto hurac sandy recuerda cula independe...  \n",
       "3  fake                       scary shit hurricane ny http  \n",
       "4  fake  fave place world nyc hurricane sandy statueofl...  \n",
       "5  fake         42nd time square nyc subway hurricane http  \n",
       "6  fake  time halloween photo hurricane sandy frankenst...  \n",
       "7  fake  crazy hurricane sandy prayer family friend eas...  \n",
       "8  fake   sandy newyork hurricane statueofliberty usa http  \n",
       "9  fake                                 nyc hurricane http  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21dd771",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "eta0 must be > 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#sgd \u001b[39;00m\n\u001b[1;32m     10\u001b[0m lr \u001b[38;5;241m=\u001b[39m SGDClassifier(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m,learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madaptive\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[0;32m---> 11\u001b[0m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m SGD_accuracy \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mscore(X_test_vec, y_test)\n\u001b[1;32m     13\u001b[0m SGD_F1 \u001b[38;5;241m=\u001b[39m  calcF1(lr\u001b[38;5;241m.\u001b[39mpredict(X_test_vec),y_test)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:890\u001b[0m, in \u001b[0;36mBaseSGDClassifier.fit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, coef_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, intercept_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit linear model with Stochastic Gradient Descent.\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m        Returns an instance of self.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:658\u001b[0m, in \u001b[0;36mBaseSGDClassifier._fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    648\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    656\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    657\u001b[0m ):\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;66;03m# delete the attribute otherwise _partial_fit thinks it's not the first call\u001b[39;00m\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28mdelattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:148\u001b[0m, in \u001b[0;36mBaseSGD._validate_params\u001b[0;34m(self, for_partial_fit)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvscaling\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madaptive\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta0 \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta0 must be > 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha must be > 0 since \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. alpha is used \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto compute the optimal learning rate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: eta0 must be > 0"
     ]
    }
   ],
   "source": [
    "X_train = df_train['pTweets']\n",
    "X_test = df_test['pTweets']\n",
    "y_train = df_train['label']\n",
    "y_test= df_test['label']\n",
    "vectorizer = TfidfVectorizer( max_df=0.3)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "#sgd \n",
    "lr = SGDClassifier(max_iter=2000,alpha=0.0001,learning_rate='adaptive');\n",
    "lr.fit(X_train_vec, y_train)\n",
    "SGD_accuracy = lr.score(X_test_vec, y_test)\n",
    "SGD_F1 =  calcF1(lr.predict(X_test_vec),y_test)\n",
    "\n",
    "#LR\n",
    "lr = LogisticRegression(C=100, max_iter=1000)\n",
    "lr.fit(X_train_vec, y_train)\n",
    "LG_accuracy = lr.score(X_test_vec, y_test)\n",
    "LG_F1 =  calcF1(lr.predict(X_test_vec),y_test)\n",
    "\n",
    "#SVM\n",
    "clf = SVC(kernel='poly')\n",
    "clf.fit(X_train_vec, y_train)\n",
    "SVM_accuracy = clf.score(X_test_vec, y_test)\n",
    "SVM_F1 = calcF1(clf.predict(X_test_vec),y_test)\n",
    "\n",
    "#DT\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train_vec, y_train)\n",
    "DT_accuracy = dt.score(X_test_vec, y_test)\n",
    "DT_F1 = calcF1(dt.predict(X_test_vec),y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fa0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_axis = ['Decision Tree', 'SVM', 'Logistic Regression', 'SGD']\n",
    "y_axis = [DT_F1, SVM_F1, LG_F1, SGD_F1]\n",
    "\n",
    "plt.bar(x_axis, y_axis)\n",
    "plt.title('F1 Scores with TF-IDF  & Preprocessing ')\n",
    "plt.xlabel('Algorithms')\n",
    "plt.ylabel('F1 ')\n",
    "for i, v in enumerate(y_axis):\n",
    "    plt.text(i-0.25, v+0.01, \"{:.3f}\".format(v))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9deb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = ['Decision Tree', 'SVM', 'Logistic Regression', 'SGD']\n",
    "y_axis = [DT_accuracy, SVM_accuracy, LG_accuracy, SGD_accuracy]\n",
    "\n",
    "plt.bar(x_axis, y_axis)\n",
    "plt.title('Accuracy with TF-IDF  & Preprocessing')\n",
    "plt.xlabel('Algorithms')\n",
    "plt.ylabel('Accuracy')\n",
    "for i, v in enumerate(y_axis):\n",
    "    plt.text(i-0.25, v+0.01, \"{:.3f}\".format(v))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd010bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29968e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

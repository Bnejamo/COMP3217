{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c5a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa85742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "\n",
    "# ignore all warnings\n",
    "simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2811f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"TrainingDataMulti.csv\", sep=\",\",header=None)\n",
    "testData = pd.read_csv(\"TestingDataMulti.csv\", sep=\",\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d76668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data = trainData)\n",
    "df_test = pd.DataFrame(data = testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed8ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.399324</td>\n",
       "      <td>127673.0908</td>\n",
       "      <td>-49.572308</td>\n",
       "      <td>127648.0176</td>\n",
       "      <td>-169.578319</td>\n",
       "      <td>127723.2374</td>\n",
       "      <td>65.689611</td>\n",
       "      <td>605.91099</td>\n",
       "      <td>-57.003571</td>\n",
       "      <td>626.78553</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.688102</td>\n",
       "      <td>130280.7109</td>\n",
       "      <td>-46.300719</td>\n",
       "      <td>130255.6377</td>\n",
       "      <td>-166.278082</td>\n",
       "      <td>130355.9307</td>\n",
       "      <td>71.831719</td>\n",
       "      <td>483.59351</td>\n",
       "      <td>-50.947407</td>\n",
       "      <td>500.98896</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.733939</td>\n",
       "      <td>130305.7842</td>\n",
       "      <td>-46.254883</td>\n",
       "      <td>130280.7109</td>\n",
       "      <td>-166.232245</td>\n",
       "      <td>130381.0040</td>\n",
       "      <td>71.808800</td>\n",
       "      <td>483.59351</td>\n",
       "      <td>-50.913030</td>\n",
       "      <td>500.98896</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.083443</td>\n",
       "      <td>130581.5902</td>\n",
       "      <td>-45.899649</td>\n",
       "      <td>130556.5169</td>\n",
       "      <td>-165.882741</td>\n",
       "      <td>130656.8100</td>\n",
       "      <td>72.152575</td>\n",
       "      <td>482.86107</td>\n",
       "      <td>-50.437475</td>\n",
       "      <td>499.15786</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.553268</td>\n",
       "      <td>131083.0556</td>\n",
       "      <td>-45.424094</td>\n",
       "      <td>131057.9823</td>\n",
       "      <td>-165.424375</td>\n",
       "      <td>131158.2754</td>\n",
       "      <td>72.118198</td>\n",
       "      <td>484.50906</td>\n",
       "      <td>-50.013486</td>\n",
       "      <td>497.69298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0            1          2            3           4            5    \\\n",
       "0  70.399324  127673.0908 -49.572308  127648.0176 -169.578319  127723.2374   \n",
       "1  73.688102  130280.7109 -46.300719  130255.6377 -166.278082  130355.9307   \n",
       "2  73.733939  130305.7842 -46.254883  130280.7109 -166.232245  130381.0040   \n",
       "3  74.083443  130581.5902 -45.899649  130556.5169 -165.882741  130656.8100   \n",
       "4  74.553268  131083.0556 -45.424094  131057.9823 -165.424375  131158.2754   \n",
       "\n",
       "         6          7          8          9    ...  119  120  121  122  123  \\\n",
       "0  65.689611  605.91099 -57.003571  626.78553  ...    0    0    0    0    0   \n",
       "1  71.831719  483.59351 -50.947407  500.98896  ...    0    0    0    0    0   \n",
       "2  71.808800  483.59351 -50.913030  500.98896  ...    0    0    0    0    0   \n",
       "3  72.152575  482.86107 -50.437475  499.15786  ...    0    0    0    0    0   \n",
       "4  72.118198  484.50906 -50.013486  497.69298  ...    0    0    0    0    0   \n",
       "\n",
       "   124  125  126  127  128  \n",
       "0    0    0    0    0    0  \n",
       "1    0    0    0    0    0  \n",
       "2    0    0    0    0    0  \n",
       "3    0    0    0    0    0  \n",
       "4    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3478acfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.399324</td>\n",
       "      <td>127673.0908</td>\n",
       "      <td>-49.572308</td>\n",
       "      <td>127648.0176</td>\n",
       "      <td>-169.578319</td>\n",
       "      <td>127723.2374</td>\n",
       "      <td>65.689611</td>\n",
       "      <td>605.91099</td>\n",
       "      <td>-57.003571</td>\n",
       "      <td>626.78553</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.688102</td>\n",
       "      <td>130280.7109</td>\n",
       "      <td>-46.300719</td>\n",
       "      <td>130255.6377</td>\n",
       "      <td>-166.278082</td>\n",
       "      <td>130355.9307</td>\n",
       "      <td>71.831719</td>\n",
       "      <td>483.59351</td>\n",
       "      <td>-50.947407</td>\n",
       "      <td>500.98896</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.733939</td>\n",
       "      <td>130305.7842</td>\n",
       "      <td>-46.254883</td>\n",
       "      <td>130280.7109</td>\n",
       "      <td>-166.232245</td>\n",
       "      <td>130381.0040</td>\n",
       "      <td>71.808800</td>\n",
       "      <td>483.59351</td>\n",
       "      <td>-50.913030</td>\n",
       "      <td>500.98896</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.083443</td>\n",
       "      <td>130581.5902</td>\n",
       "      <td>-45.899649</td>\n",
       "      <td>130556.5169</td>\n",
       "      <td>-165.882741</td>\n",
       "      <td>130656.8100</td>\n",
       "      <td>72.152575</td>\n",
       "      <td>482.86107</td>\n",
       "      <td>-50.437475</td>\n",
       "      <td>499.15786</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.553268</td>\n",
       "      <td>131083.0556</td>\n",
       "      <td>-45.424094</td>\n",
       "      <td>131057.9823</td>\n",
       "      <td>-165.424375</td>\n",
       "      <td>131158.2754</td>\n",
       "      <td>72.118198</td>\n",
       "      <td>484.50906</td>\n",
       "      <td>-50.013486</td>\n",
       "      <td>497.69298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>116.889120</td>\n",
       "      <td>131860.3269</td>\n",
       "      <td>-3.076783</td>\n",
       "      <td>131810.1804</td>\n",
       "      <td>-123.094253</td>\n",
       "      <td>131910.4735</td>\n",
       "      <td>114.780635</td>\n",
       "      <td>376.10794</td>\n",
       "      <td>-5.254023</td>\n",
       "      <td>374.82617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>116.849013</td>\n",
       "      <td>131810.1804</td>\n",
       "      <td>-3.116890</td>\n",
       "      <td>131760.0339</td>\n",
       "      <td>-123.128630</td>\n",
       "      <td>131885.4002</td>\n",
       "      <td>114.769176</td>\n",
       "      <td>376.29105</td>\n",
       "      <td>-5.322778</td>\n",
       "      <td>374.82617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>116.384917</td>\n",
       "      <td>131734.9606</td>\n",
       "      <td>-3.586716</td>\n",
       "      <td>131684.8140</td>\n",
       "      <td>-123.586996</td>\n",
       "      <td>131785.1071</td>\n",
       "      <td>114.299351</td>\n",
       "      <td>376.47416</td>\n",
       "      <td>-5.849899</td>\n",
       "      <td>374.82617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>111.125164</td>\n",
       "      <td>130506.3704</td>\n",
       "      <td>-8.846468</td>\n",
       "      <td>130456.2238</td>\n",
       "      <td>-128.858208</td>\n",
       "      <td>130556.5169</td>\n",
       "      <td>106.667553</td>\n",
       "      <td>478.83265</td>\n",
       "      <td>-13.464508</td>\n",
       "      <td>477.73399</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>110.878793</td>\n",
       "      <td>130481.2971</td>\n",
       "      <td>-9.092840</td>\n",
       "      <td>130456.2238</td>\n",
       "      <td>-129.104580</td>\n",
       "      <td>130556.5169</td>\n",
       "      <td>106.392533</td>\n",
       "      <td>478.83265</td>\n",
       "      <td>-13.750987</td>\n",
       "      <td>477.91710</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1          2            3           4    \\\n",
       "0      70.399324  127673.0908 -49.572308  127648.0176 -169.578319   \n",
       "1      73.688102  130280.7109 -46.300719  130255.6377 -166.278082   \n",
       "2      73.733939  130305.7842 -46.254883  130280.7109 -166.232245   \n",
       "3      74.083443  130581.5902 -45.899649  130556.5169 -165.882741   \n",
       "4      74.553268  131083.0556 -45.424094  131057.9823 -165.424375   \n",
       "...          ...          ...        ...          ...         ...   \n",
       "5995  116.889120  131860.3269  -3.076783  131810.1804 -123.094253   \n",
       "5996  116.849013  131810.1804  -3.116890  131760.0339 -123.128630   \n",
       "5997  116.384917  131734.9606  -3.586716  131684.8140 -123.586996   \n",
       "5998  111.125164  130506.3704  -8.846468  130456.2238 -128.858208   \n",
       "5999  110.878793  130481.2971  -9.092840  130456.2238 -129.104580   \n",
       "\n",
       "              5           6          7          8          9    ...  118  119  \\\n",
       "0     127723.2374   65.689611  605.91099 -57.003571  626.78553  ...    0    0   \n",
       "1     130355.9307   71.831719  483.59351 -50.947407  500.98896  ...    0    0   \n",
       "2     130381.0040   71.808800  483.59351 -50.913030  500.98896  ...    0    0   \n",
       "3     130656.8100   72.152575  482.86107 -50.437475  499.15786  ...    0    0   \n",
       "4     131158.2754   72.118198  484.50906 -50.013486  497.69298  ...    0    0   \n",
       "...           ...         ...        ...        ...        ...  ...  ...  ...   \n",
       "5995  131910.4735  114.780635  376.10794  -5.254023  374.82617  ...    0    0   \n",
       "5996  131885.4002  114.769176  376.29105  -5.322778  374.82617  ...    0    0   \n",
       "5997  131785.1071  114.299351  376.47416  -5.849899  374.82617  ...    0    0   \n",
       "5998  130556.5169  106.667553  478.83265 -13.464508  477.73399  ...    0    0   \n",
       "5999  130556.5169  106.392533  478.83265 -13.750987  477.91710  ...    0    0   \n",
       "\n",
       "      120  121  122  123  124  125  126  127  \n",
       "0       0    0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0    0  \n",
       "3       0    0    0    0    0    0    0    0  \n",
       "4       0    0    0    0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "5995    0    0    0    0    0    0    0    0  \n",
       "5996    0    0    0    0    0    0    0    0  \n",
       "5997    0    0    0    0    0    0    0    0  \n",
       "5998    0    0    0    0    0    0    0    0  \n",
       "5999    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[6000 rows x 128 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0501fc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5995    0\n",
       "5996    0\n",
       "5997    0\n",
       "5998    0\n",
       "5999    0\n",
       "Name: 128, Length: 6000, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd95ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3000\n",
       "2    1500\n",
       "1    1500\n",
       "Name: 128, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[:,-1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d840a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1],df_train.iloc[:,-1], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5013aa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), SGDClassifier())\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Finally, we can evaluate the model's performance on the test set\n",
    "SGD_accuracy = pipe.score(X_test, y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(SGD_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad54470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = make_pipeline(StandardScaler(), SVC(kernel='poly'))\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Finally, we can evaluate the model's performance on the test set\n",
    "SVM_accuracy = pipe.score(X_test, y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(SVM_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97d5ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(StandardScaler(),LogisticRegression(C=100, max_iter=1000))\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Finally, we can evaluate the model's performance on the test set\n",
    "LG_accuracy = pipe.score(X_test, y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(LG_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75449927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(StandardScaler(),DecisionTreeClassifier())\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Finally, we can evaluate the model's performance on the test set\n",
    "DT_accuracy = pipe.score(X_test, y_test)\n",
    "pipe\n",
    "print(\"Test set accuracy: {:.2f}\".format(DT_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "610919d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = ['Decision Tree', 'SVM', 'Logistic Regression', 'SGD']\n",
    "y_axis = [DT_accuracy, SVM_accuracy, LG_accuracy, SGD_accuracy]\n",
    "\n",
    "plt.bar(x_axis, y_axis)\n",
    "plt.title('Accuracy with Scaling')\n",
    "plt.xlabel('Algorithms')\n",
    "plt.ylabel('Accuracy')\n",
    "for i, v in enumerate(y_axis):\n",
    "    plt.text(i-0.25, v+0.01, \"{:.3f}\".format(v))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e3a8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldAverages(num,scalar):\n",
    "    SGD_accuracy =[]\n",
    "    SVM_accuracy =[]\n",
    "    LG_accuracy =[]\n",
    "    DT_accuracy =[]\n",
    "    RF_accuracy =[]\n",
    "    BG_accuracy =[]\n",
    "    ADA_accuracy = []\n",
    "    kf = KFold(n_splits=num,shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(df_train)):\n",
    "        print(f\"Fold {i}:\")\n",
    "#         print(f\"  Train: index={train_index}\") \n",
    "#         print(f\"  Test:  index={test_index}\")\n",
    "        X_train=df_train.iloc[train_index,:-1]\n",
    "        X_test=df_train.iloc[test_index,:-1]\n",
    "        y_train=df_train.iloc[train_index,-1]\n",
    "        y_test=df_train.iloc[test_index,-1]\n",
    "        \n",
    "        pipe = make_pipeline(scalar(), SGDClassifier())\n",
    "        pipe.fit(X_train, y_train)\n",
    "        SGD_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        pipe = make_pipeline(scalar(), SVC(kernel='poly'))\n",
    "        pipe.fit(X_train, y_train)\n",
    "        SVM_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        pipe = make_pipeline(StandardScaler(),LogisticRegression(C=100, max_iter=1000))\n",
    "        pipe.fit(X_train, y_train)\n",
    "        LG_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        pipe = make_pipeline(StandardScaler(),DecisionTreeClassifier())\n",
    "        pipe.fit(X_train, y_train)\n",
    "        DT_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        pipe = make_pipeline(StandardScaler(),AdaBoostClassifier(base_estimator=DecisionTreeClassifier()))\n",
    "        pipe.fit(X_train, y_train)\n",
    "        ADA_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        \n",
    "        pipe = make_pipeline(StandardScaler(),RandomForestClassifier())\n",
    "        pipe.fit(X_train, y_train)\n",
    "        RF_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(), n_estimators=500,\n",
    "            max_samples=1000, bootstrap=True, n_jobs=-1\n",
    "        )\n",
    "        bag_clf.fit(X_train, y_train)\n",
    "        BG_accuracy.append(bag_clf.score(X_test, y_test))\n",
    "    \n",
    "    print(\"SGD Test set accuracy\", SGD_accuracy) \n",
    "    print(\"SVM Test set accuracy\", SVM_accuracy) \n",
    "    print(\"LG Test set accuracy\", LG_accuracy) \n",
    "    print(\"DT Test set accuracy\", DT_accuracy) \n",
    "    print(\"BG Test set accuracy\", BG_accuracy) \n",
    "    x_axis = ['Decision Tree','Random Forest','Bagging Classifier','SVM' ,'ADA', 'Logistic Regression', 'SGD']\n",
    "    y_axis = [DT_accuracy,RF_accuracy , BG_accuracy ,SVM_accuracy, ADA_accuracy ,LG_accuracy, SGD_accuracy]\n",
    "    y_axis=[sum(y) / len(y) for y in y_axis]\n",
    "    plt.bar(x_axis, y_axis)\n",
    "    plt.title('Accuracy with Scaling')\n",
    "    plt.xlabel('Algorithms')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, v in enumerate(y_axis):\n",
    "        plt.text(i-0.25, v+0.01, \"{:.3f}\".format(v))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d869215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Fold 1:\n",
      "Fold 2:\n",
      "Fold 3:\n",
      "Fold 4:\n",
      "SGD Test set accuracy [0.6341666666666667, 0.6991666666666667, 0.6833333333333333, 0.6941666666666667, 0.6866666666666666]\n",
      "SVM Test set accuracy [0.6416666666666667, 0.6391666666666667, 0.6558333333333334, 0.6241666666666666, 0.6616666666666666]\n",
      "LG Test set accuracy [0.7108333333333333, 0.7433333333333333, 0.7458333333333333, 0.725, 0.7408333333333333]\n",
      "DT Test set accuracy [0.8991666666666667, 0.8833333333333333, 0.9116666666666666, 0.8858333333333334, 0.8966666666666666]\n",
      "BG Test set accuracy [0.8866666666666667, 0.8808333333333334, 0.8966666666666666, 0.9066666666666666, 0.9058333333333334]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kFoldAverages(5,StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f764903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldBoxAverages(num,scalar):\n",
    "    SGD_accuracy =[]\n",
    "    SVM_accuracy =[]\n",
    "    LG_accuracy =[]\n",
    "    DT_accuracy =[]\n",
    "    RF_accuracy = []\n",
    "    kf = KFold(n_splits=num,shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(df_train)):\n",
    "#         print(f\"Fold {i}:\")\n",
    "#         print(f\"  Train: index={train_index}\") \n",
    "#         print(f\"  Test:  index={test_index}\")\n",
    "        X_train=df_train.iloc[train_index,:-1]\n",
    "        X_test=df_train.iloc[test_index,:-1]\n",
    "        y_train=df_train.iloc[train_index,-1]\n",
    "        y_test=df_train.iloc[test_index,-1]\n",
    "        \n",
    "        pipe = make_pipeline(scalar(), SGDClassifier())\n",
    "        pipe.fit(X_train, y_train)\n",
    "        SGD_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        pipe = make_pipeline(scalar(), SVC(kernel='poly'))\n",
    "        pipe.fit(X_train, y_train)\n",
    "        SVM_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        pipe = make_pipeline(StandardScaler(),LogisticRegression(C=100, max_iter=1000))\n",
    "        pipe.fit(X_train, y_train)\n",
    "        LG_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        pipe = make_pipeline(StandardScaler(),DecisionTreeClassifier())\n",
    "        pipe.fit(X_train, y_train)\n",
    "        DT_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "        pipe = make_pipeline(StandardScaler(),RandomForestClassifier())\n",
    "        pipe.fit(X_train, y_train)\n",
    "        RF_accuracy.append(pipe.score(X_test, y_test)) \n",
    "                   \n",
    "\n",
    "    \n",
    "#     print(\"SGD Test set accuracy\", SGD_accuracy) \n",
    "#     print(\"SVM Test set accuracy\", SVM_accuracy) \n",
    "#     print(\"LG Test set accuracy\", LG_accuracy) \n",
    "#     print(\"DT Test set accuracy\", DT_accuracy) \n",
    "    plt.title('Accuracy with Scaling')\n",
    "    plt.xlabel('Algorithms')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.boxplot([DT_accuracy,RF_accuracy,SVM_accuracy,SGD_accuracy,LG_accuracy],labels= ['Decision Tree','Random Forest', 'SVM', 'Logistic Regression', 'SGD'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867dba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1],df_train.iloc[:,-1], test_size=0.1, random_state=10)\n",
    "pipe = make_pipeline(StandardScaler(),DecisionTreeClassifier())\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Finally, we can evaluate the model's performance on the test set\n",
    "DT_accuracy = pipe.score(X_test, y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(DT_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a0fe7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "        estimators=[('lr', LogisticRegression()), ('rf', RandomForestClassifier())],\n",
    "        voting='hard'\n",
    "    )\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(),RandomForestClassifier())\n",
    "pipe.fit(X_train, y_train)\n",
    "                   \n",
    "\n",
    "# Finally, we can evaluate the model's performance on the test set\n",
    "DT_accuracy = pipe.score(X_test, y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(DT_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f99f9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1],df_train.iloc[:,-1], test_size=0.1, random_state=10)\n",
    "\n",
    "\n",
    "print(\"Test set accuracy: {:.2f}\".format(DT_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eeeda8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:36:31.084287: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 1s 2ms/step - loss: 0.7570 - accuracy: 0.6598 - val_loss: 0.6025 - val_accuracy: 0.7483\n",
      "Epoch 2/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.5925 - accuracy: 0.7459 - val_loss: 0.5511 - val_accuracy: 0.7800\n",
      "Epoch 3/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.5245 - accuracy: 0.7754 - val_loss: 0.5895 - val_accuracy: 0.7483\n",
      "Epoch 4/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.4840 - accuracy: 0.7956 - val_loss: 0.4988 - val_accuracy: 0.7950\n",
      "Epoch 5/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.4533 - accuracy: 0.8080 - val_loss: 0.4784 - val_accuracy: 0.8150\n",
      "Epoch 6/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8269 - val_loss: 0.4909 - val_accuracy: 0.8050\n",
      "Epoch 7/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.4093 - accuracy: 0.8356 - val_loss: 0.4837 - val_accuracy: 0.8150\n",
      "Epoch 8/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.3779 - accuracy: 0.8494 - val_loss: 0.3983 - val_accuracy: 0.8500\n",
      "Epoch 9/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8550 - val_loss: 0.4636 - val_accuracy: 0.8100\n",
      "Epoch 10/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8617 - val_loss: 0.4117 - val_accuracy: 0.8467\n",
      "Epoch 11/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.3226 - accuracy: 0.8667 - val_loss: 0.3972 - val_accuracy: 0.8483\n",
      "Epoch 12/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.3104 - accuracy: 0.8759 - val_loss: 0.3806 - val_accuracy: 0.8283\n",
      "Epoch 13/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2834 - accuracy: 0.8872 - val_loss: 0.4018 - val_accuracy: 0.8667\n",
      "Epoch 14/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2670 - accuracy: 0.8931 - val_loss: 0.4071 - val_accuracy: 0.8283\n",
      "Epoch 15/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2618 - accuracy: 0.8974 - val_loss: 0.3553 - val_accuracy: 0.8617\n",
      "Epoch 16/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.9006 - val_loss: 0.3763 - val_accuracy: 0.8583\n",
      "Epoch 17/80\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.8976 - val_loss: 0.3553 - val_accuracy: 0.8733\n",
      "Epoch 18/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9057 - val_loss: 0.3282 - val_accuracy: 0.8850\n",
      "Epoch 19/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9124 - val_loss: 0.3306 - val_accuracy: 0.8883\n",
      "Epoch 20/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9157 - val_loss: 0.3028 - val_accuracy: 0.9033\n",
      "Epoch 21/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1901 - accuracy: 0.9237 - val_loss: 0.3021 - val_accuracy: 0.8833\n",
      "Epoch 22/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9276 - val_loss: 0.3165 - val_accuracy: 0.9033\n",
      "Epoch 23/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9285 - val_loss: 0.3608 - val_accuracy: 0.8867\n",
      "Epoch 24/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.9302 - val_loss: 0.3697 - val_accuracy: 0.8900\n",
      "Epoch 25/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9333 - val_loss: 0.3837 - val_accuracy: 0.8983\n",
      "Epoch 26/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9370 - val_loss: 0.3190 - val_accuracy: 0.8950\n",
      "Epoch 27/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9350 - val_loss: 0.3112 - val_accuracy: 0.8983\n",
      "Epoch 28/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9339 - val_loss: 0.3288 - val_accuracy: 0.8850\n",
      "Epoch 29/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9331 - val_loss: 0.3705 - val_accuracy: 0.8867\n",
      "Epoch 30/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9411 - val_loss: 0.2704 - val_accuracy: 0.9217\n",
      "Epoch 31/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9493 - val_loss: 0.3025 - val_accuracy: 0.9083\n",
      "Epoch 32/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9494 - val_loss: 0.3306 - val_accuracy: 0.9167\n",
      "Epoch 33/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9539 - val_loss: 0.3245 - val_accuracy: 0.9200\n",
      "Epoch 34/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9578 - val_loss: 0.2633 - val_accuracy: 0.9167\n",
      "Epoch 35/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9570 - val_loss: 0.3631 - val_accuracy: 0.8933\n",
      "Epoch 36/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9513 - val_loss: 0.3211 - val_accuracy: 0.8983\n",
      "Epoch 37/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9519 - val_loss: 0.2530 - val_accuracy: 0.9250\n",
      "Epoch 38/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9637 - val_loss: 0.3300 - val_accuracy: 0.9150\n",
      "Epoch 39/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9589 - val_loss: 0.3775 - val_accuracy: 0.9033\n",
      "Epoch 40/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9559 - val_loss: 0.4054 - val_accuracy: 0.8950\n",
      "Epoch 41/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9554 - val_loss: 0.3338 - val_accuracy: 0.9333\n",
      "Epoch 42/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9593 - val_loss: 0.2646 - val_accuracy: 0.9217\n",
      "Epoch 43/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9635 - val_loss: 0.3028 - val_accuracy: 0.9217\n",
      "Epoch 44/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9593 - val_loss: 0.3816 - val_accuracy: 0.9133\n",
      "Epoch 45/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9661 - val_loss: 0.3448 - val_accuracy: 0.9267\n",
      "Epoch 46/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9641 - val_loss: 0.2951 - val_accuracy: 0.9300\n",
      "Epoch 47/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9615 - val_loss: 0.3119 - val_accuracy: 0.8983\n",
      "Epoch 48/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9674 - val_loss: 0.2649 - val_accuracy: 0.9283\n",
      "Epoch 49/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9661 - val_loss: 0.3327 - val_accuracy: 0.9233\n",
      "Epoch 50/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9670 - val_loss: 0.3732 - val_accuracy: 0.9050\n",
      "Epoch 51/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9643 - val_loss: 0.3159 - val_accuracy: 0.9333\n",
      "Epoch 52/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9646 - val_loss: 0.3204 - val_accuracy: 0.9100\n",
      "Epoch 53/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9600 - val_loss: 0.3509 - val_accuracy: 0.9233\n",
      "Epoch 54/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9735 - val_loss: 0.3688 - val_accuracy: 0.9300\n",
      "Epoch 55/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9733 - val_loss: 0.3760 - val_accuracy: 0.9183\n",
      "Epoch 56/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9741 - val_loss: 0.4094 - val_accuracy: 0.9250\n",
      "Epoch 57/80\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.0966 - accuracy: 0.9646 - val_loss: 0.2908 - val_accuracy: 0.9200\n",
      "Epoch 58/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9715 - val_loss: 0.2949 - val_accuracy: 0.9233\n",
      "Epoch 59/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9648 - val_loss: 0.3362 - val_accuracy: 0.9367\n",
      "Epoch 60/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9726 - val_loss: 0.2766 - val_accuracy: 0.9367\n",
      "Epoch 61/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9726 - val_loss: 0.2906 - val_accuracy: 0.9283\n",
      "Epoch 62/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9763 - val_loss: 0.3038 - val_accuracy: 0.9400\n",
      "Epoch 63/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9743 - val_loss: 0.3302 - val_accuracy: 0.9300\n",
      "Epoch 64/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9791 - val_loss: 0.3612 - val_accuracy: 0.9283\n",
      "Epoch 65/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9720 - val_loss: 0.3106 - val_accuracy: 0.9233\n",
      "Epoch 66/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9700 - val_loss: 0.3313 - val_accuracy: 0.9200\n",
      "Epoch 67/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9687 - val_loss: 0.3585 - val_accuracy: 0.9250\n",
      "Epoch 68/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9685 - val_loss: 0.2756 - val_accuracy: 0.9233\n",
      "Epoch 69/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9733 - val_loss: 0.2710 - val_accuracy: 0.9333\n",
      "Epoch 70/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9757 - val_loss: 0.3517 - val_accuracy: 0.9200\n",
      "Epoch 71/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9741 - val_loss: 0.3302 - val_accuracy: 0.9250\n",
      "Epoch 72/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0556 - accuracy: 0.9793 - val_loss: 0.3208 - val_accuracy: 0.9267\n",
      "Epoch 73/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0463 - accuracy: 0.9817 - val_loss: 0.3513 - val_accuracy: 0.9267\n",
      "Epoch 74/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.9794 - val_loss: 0.3392 - val_accuracy: 0.9300\n",
      "Epoch 75/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0554 - accuracy: 0.9785 - val_loss: 0.3725 - val_accuracy: 0.9250\n",
      "Epoch 76/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9687 - val_loss: 0.3065 - val_accuracy: 0.9333\n",
      "Epoch 77/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9794 - val_loss: 0.2744 - val_accuracy: 0.9400\n",
      "Epoch 78/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.9819 - val_loss: 0.3559 - val_accuracy: 0.9283\n",
      "Epoch 79/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9763 - val_loss: 0.3692 - val_accuracy: 0.9300\n",
      "Epoch 80/80\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9807 - val_loss: 0.4027 - val_accuracy: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1764c7940>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1],df_train.iloc[:,-1], test_size=0.1, random_state=10)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(256, activation='relu', input_shape=(128,)),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=80, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9784eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "169/169 [==============================] - 1s 2ms/step - loss: 11.3477 - accuracy: 0.5919 - val_loss: 4.3551 - val_accuracy: 0.6417\n",
      "Epoch 2/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2.6045 - accuracy: 0.6519 - val_loss: 1.5403 - val_accuracy: 0.6467\n",
      "Epoch 3/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 1.3876 - accuracy: 0.6719 - val_loss: 1.3000 - val_accuracy: 0.6667\n",
      "Epoch 4/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 1.1668 - accuracy: 0.6789 - val_loss: 1.0579 - val_accuracy: 0.6833\n",
      "Epoch 5/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 1.0809 - accuracy: 0.6844 - val_loss: 1.0973 - val_accuracy: 0.6950\n",
      "Epoch 6/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 1.0201 - accuracy: 0.6926 - val_loss: 0.9656 - val_accuracy: 0.7033\n",
      "Epoch 7/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.9655 - accuracy: 0.6976 - val_loss: 0.9015 - val_accuracy: 0.6983\n",
      "Epoch 8/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.9126 - accuracy: 0.7019 - val_loss: 0.8641 - val_accuracy: 0.7017\n",
      "Epoch 9/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8860 - accuracy: 0.7054 - val_loss: 0.9001 - val_accuracy: 0.6917\n",
      "Epoch 10/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8829 - accuracy: 0.7176 - val_loss: 0.8493 - val_accuracy: 0.7067\n",
      "Epoch 11/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8830 - accuracy: 0.7104 - val_loss: 0.8848 - val_accuracy: 0.7100\n",
      "Epoch 12/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8602 - accuracy: 0.7141 - val_loss: 0.8827 - val_accuracy: 0.7183\n",
      "Epoch 13/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8548 - accuracy: 0.7180 - val_loss: 0.8495 - val_accuracy: 0.7100\n",
      "Epoch 14/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8507 - accuracy: 0.7215 - val_loss: 0.8314 - val_accuracy: 0.7133\n",
      "Epoch 15/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8528 - accuracy: 0.7194 - val_loss: 0.8302 - val_accuracy: 0.7200\n",
      "Epoch 16/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8266 - accuracy: 0.7265 - val_loss: 0.8422 - val_accuracy: 0.7067\n",
      "Epoch 17/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8454 - accuracy: 0.7213 - val_loss: 0.8407 - val_accuracy: 0.7183\n",
      "Epoch 18/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8255 - accuracy: 0.7306 - val_loss: 0.7765 - val_accuracy: 0.7417\n",
      "Epoch 19/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8341 - accuracy: 0.7352 - val_loss: 0.8015 - val_accuracy: 0.7233\n",
      "Epoch 20/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8266 - accuracy: 0.7293 - val_loss: 0.7784 - val_accuracy: 0.7367\n",
      "Epoch 21/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8085 - accuracy: 0.7263 - val_loss: 0.8035 - val_accuracy: 0.7300\n",
      "Epoch 22/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7821 - accuracy: 0.7461 - val_loss: 0.7770 - val_accuracy: 0.7483\n",
      "Epoch 23/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7801 - accuracy: 0.7380 - val_loss: 0.8147 - val_accuracy: 0.7067\n",
      "Epoch 24/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7948 - accuracy: 0.7439 - val_loss: 0.7904 - val_accuracy: 0.7450\n",
      "Epoch 25/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8243 - accuracy: 0.7343 - val_loss: 0.7708 - val_accuracy: 0.7583\n",
      "Epoch 26/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.8016 - accuracy: 0.7435 - val_loss: 0.8286 - val_accuracy: 0.7267\n",
      "Epoch 27/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7859 - accuracy: 0.7415 - val_loss: 0.7562 - val_accuracy: 0.7250\n",
      "Epoch 28/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7674 - accuracy: 0.7452 - val_loss: 0.7389 - val_accuracy: 0.7400\n",
      "Epoch 29/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7848 - accuracy: 0.7411 - val_loss: 0.7766 - val_accuracy: 0.7383\n",
      "Epoch 30/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7712 - accuracy: 0.7524 - val_loss: 0.7783 - val_accuracy: 0.7333\n",
      "Epoch 31/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7984 - accuracy: 0.7446 - val_loss: 0.7342 - val_accuracy: 0.7567\n",
      "Epoch 32/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7617 - accuracy: 0.7507 - val_loss: 0.7360 - val_accuracy: 0.7617\n",
      "Epoch 33/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7649 - accuracy: 0.7459 - val_loss: 0.7550 - val_accuracy: 0.7583\n",
      "Epoch 34/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7608 - accuracy: 0.7494 - val_loss: 0.8116 - val_accuracy: 0.7400\n",
      "Epoch 35/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7473 - accuracy: 0.7520 - val_loss: 0.7590 - val_accuracy: 0.7600\n",
      "Epoch 36/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7515 - accuracy: 0.7559 - val_loss: 0.7572 - val_accuracy: 0.7567\n",
      "Epoch 37/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7480 - accuracy: 0.7578 - val_loss: 0.7351 - val_accuracy: 0.7850\n",
      "Epoch 38/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7394 - accuracy: 0.7581 - val_loss: 0.7914 - val_accuracy: 0.7800\n",
      "Epoch 39/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7256 - accuracy: 0.7665 - val_loss: 0.7419 - val_accuracy: 0.7633\n",
      "Epoch 40/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7541 - accuracy: 0.7580 - val_loss: 0.7340 - val_accuracy: 0.7617\n",
      "Epoch 41/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7287 - accuracy: 0.7635 - val_loss: 0.6996 - val_accuracy: 0.7967\n",
      "Epoch 42/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7152 - accuracy: 0.7737 - val_loss: 0.6903 - val_accuracy: 0.8050\n",
      "Epoch 43/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.7674 - val_loss: 0.6790 - val_accuracy: 0.7833\n",
      "Epoch 44/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7038 - accuracy: 0.7685 - val_loss: 0.6763 - val_accuracy: 0.8050\n",
      "Epoch 45/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7107 - accuracy: 0.7711 - val_loss: 0.6959 - val_accuracy: 0.7783\n",
      "Epoch 46/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7225 - accuracy: 0.7656 - val_loss: 0.7672 - val_accuracy: 0.8067\n",
      "Epoch 47/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6842 - accuracy: 0.7731 - val_loss: 0.6624 - val_accuracy: 0.7733\n",
      "Epoch 48/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.7023 - accuracy: 0.7728 - val_loss: 0.6987 - val_accuracy: 0.8067\n",
      "Epoch 49/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6853 - accuracy: 0.7806 - val_loss: 0.6549 - val_accuracy: 0.8117\n",
      "Epoch 50/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.7770 - val_loss: 0.7099 - val_accuracy: 0.8000\n",
      "Epoch 51/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.7800 - val_loss: 0.7304 - val_accuracy: 0.7933\n",
      "Epoch 52/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6748 - accuracy: 0.7824 - val_loss: 0.6820 - val_accuracy: 0.8117\n",
      "Epoch 53/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6596 - accuracy: 0.7820 - val_loss: 0.6722 - val_accuracy: 0.7833\n",
      "Epoch 54/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6691 - accuracy: 0.7841 - val_loss: 0.6720 - val_accuracy: 0.7950\n",
      "Epoch 55/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6646 - accuracy: 0.7856 - val_loss: 0.6354 - val_accuracy: 0.7933\n",
      "Epoch 56/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6625 - accuracy: 0.7833 - val_loss: 0.6540 - val_accuracy: 0.8033\n",
      "Epoch 57/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.7891 - val_loss: 0.6954 - val_accuracy: 0.7950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6541 - accuracy: 0.7787 - val_loss: 0.6579 - val_accuracy: 0.8133\n",
      "Epoch 59/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6610 - accuracy: 0.7857 - val_loss: 0.6851 - val_accuracy: 0.8183\n",
      "Epoch 60/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6515 - accuracy: 0.7911 - val_loss: 0.6443 - val_accuracy: 0.8017\n",
      "Epoch 61/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.7917 - val_loss: 0.6819 - val_accuracy: 0.8133\n",
      "Epoch 62/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.7800 - val_loss: 0.6629 - val_accuracy: 0.8150\n",
      "Epoch 63/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6339 - accuracy: 0.7963 - val_loss: 0.6424 - val_accuracy: 0.8267\n",
      "Epoch 64/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6233 - accuracy: 0.7887 - val_loss: 0.6179 - val_accuracy: 0.8250\n",
      "Epoch 65/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6301 - accuracy: 0.7924 - val_loss: 0.6792 - val_accuracy: 0.8033\n",
      "Epoch 66/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6392 - accuracy: 0.7963 - val_loss: 0.6304 - val_accuracy: 0.8200\n",
      "Epoch 67/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6398 - accuracy: 0.7857 - val_loss: 0.6626 - val_accuracy: 0.8133\n",
      "Epoch 68/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.7952 - val_loss: 0.6634 - val_accuracy: 0.7833\n",
      "Epoch 69/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6188 - accuracy: 0.7965 - val_loss: 0.6242 - val_accuracy: 0.8183\n",
      "Epoch 70/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.8002 - val_loss: 0.6153 - val_accuracy: 0.8267\n",
      "Epoch 71/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.8026 - val_loss: 0.6208 - val_accuracy: 0.8200\n",
      "Epoch 72/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6051 - accuracy: 0.8056 - val_loss: 0.6012 - val_accuracy: 0.8350\n",
      "Epoch 73/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6034 - accuracy: 0.8002 - val_loss: 0.6169 - val_accuracy: 0.8267\n",
      "Epoch 74/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6070 - accuracy: 0.8043 - val_loss: 0.6314 - val_accuracy: 0.8217\n",
      "Epoch 75/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6133 - accuracy: 0.8007 - val_loss: 0.6166 - val_accuracy: 0.8250\n",
      "Epoch 76/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.8093 - val_loss: 0.7020 - val_accuracy: 0.8000\n",
      "Epoch 77/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6218 - accuracy: 0.8026 - val_loss: 0.6307 - val_accuracy: 0.8217\n",
      "Epoch 78/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5874 - accuracy: 0.8078 - val_loss: 0.6080 - val_accuracy: 0.8150\n",
      "Epoch 79/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5941 - accuracy: 0.8048 - val_loss: 0.6339 - val_accuracy: 0.8250\n",
      "Epoch 80/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5981 - accuracy: 0.8057 - val_loss: 0.6124 - val_accuracy: 0.8367\n",
      "Epoch 81/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5961 - accuracy: 0.8011 - val_loss: 0.6361 - val_accuracy: 0.8383\n",
      "Epoch 82/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.8135 - val_loss: 0.6377 - val_accuracy: 0.8017\n",
      "Epoch 83/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6037 - accuracy: 0.8048 - val_loss: 0.6396 - val_accuracy: 0.8033\n",
      "Epoch 84/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.8089 - val_loss: 0.6404 - val_accuracy: 0.8100\n",
      "Epoch 85/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.6058 - accuracy: 0.8037 - val_loss: 0.6119 - val_accuracy: 0.8400\n",
      "Epoch 86/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5881 - accuracy: 0.8124 - val_loss: 0.5923 - val_accuracy: 0.8067\n",
      "Epoch 87/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5850 - accuracy: 0.8091 - val_loss: 0.6076 - val_accuracy: 0.8367\n",
      "Epoch 88/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5918 - accuracy: 0.8087 - val_loss: 0.5975 - val_accuracy: 0.8417\n",
      "Epoch 89/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.8137 - val_loss: 0.5759 - val_accuracy: 0.8350\n",
      "Epoch 90/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.8085 - val_loss: 0.5704 - val_accuracy: 0.8400\n",
      "Epoch 91/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5896 - accuracy: 0.8087 - val_loss: 0.5932 - val_accuracy: 0.8100\n",
      "Epoch 92/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.8119 - val_loss: 0.5805 - val_accuracy: 0.8233\n",
      "Epoch 93/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5684 - accuracy: 0.8183 - val_loss: 0.5765 - val_accuracy: 0.8367\n",
      "Epoch 94/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5773 - accuracy: 0.8080 - val_loss: 0.5718 - val_accuracy: 0.8433\n",
      "Epoch 95/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5800 - accuracy: 0.8148 - val_loss: 0.5822 - val_accuracy: 0.8400\n",
      "Epoch 96/300\n",
      "169/169 [==============================] - 0s 3ms/step - loss: 0.5684 - accuracy: 0.8139 - val_loss: 0.5858 - val_accuracy: 0.8283\n",
      "Epoch 97/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5805 - accuracy: 0.8072 - val_loss: 0.6052 - val_accuracy: 0.8400\n",
      "Epoch 98/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.8265 - val_loss: 0.5862 - val_accuracy: 0.8533\n",
      "Epoch 99/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.8107 - val_loss: 0.5833 - val_accuracy: 0.8300\n",
      "Epoch 100/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5593 - accuracy: 0.8144 - val_loss: 0.5668 - val_accuracy: 0.8467\n",
      "Epoch 101/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5593 - accuracy: 0.8196 - val_loss: 0.5847 - val_accuracy: 0.8517\n",
      "Epoch 102/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5558 - accuracy: 0.8200 - val_loss: 0.5404 - val_accuracy: 0.8567\n",
      "Epoch 103/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.8200 - val_loss: 0.5408 - val_accuracy: 0.8567\n",
      "Epoch 104/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.8252 - val_loss: 0.5594 - val_accuracy: 0.8433\n",
      "Epoch 105/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.8181 - val_loss: 0.5388 - val_accuracy: 0.8517\n",
      "Epoch 106/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.8172 - val_loss: 0.5570 - val_accuracy: 0.8500\n",
      "Epoch 107/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5524 - accuracy: 0.8226 - val_loss: 0.5570 - val_accuracy: 0.8483\n",
      "Epoch 108/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5441 - accuracy: 0.8231 - val_loss: 0.5920 - val_accuracy: 0.8133\n",
      "Epoch 109/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5556 - accuracy: 0.8254 - val_loss: 0.5672 - val_accuracy: 0.8400\n",
      "Epoch 110/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5447 - accuracy: 0.8202 - val_loss: 0.5759 - val_accuracy: 0.8267\n",
      "Epoch 111/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5557 - accuracy: 0.8119 - val_loss: 0.5738 - val_accuracy: 0.8367\n",
      "Epoch 112/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5568 - accuracy: 0.8243 - val_loss: 0.5701 - val_accuracy: 0.8367\n",
      "Epoch 113/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.8211 - val_loss: 0.5845 - val_accuracy: 0.8433\n",
      "Epoch 114/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.8324 - val_loss: 0.5707 - val_accuracy: 0.8317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.8267 - val_loss: 0.5618 - val_accuracy: 0.8500\n",
      "Epoch 116/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.8281 - val_loss: 0.5611 - val_accuracy: 0.8533\n",
      "Epoch 117/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.8219 - val_loss: 0.5721 - val_accuracy: 0.8550\n",
      "Epoch 118/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5463 - accuracy: 0.8257 - val_loss: 0.5683 - val_accuracy: 0.8517\n",
      "Epoch 119/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.8269 - val_loss: 0.5744 - val_accuracy: 0.8467\n",
      "Epoch 120/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5554 - accuracy: 0.8241 - val_loss: 0.5866 - val_accuracy: 0.8333\n",
      "Epoch 121/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.8215 - val_loss: 0.5620 - val_accuracy: 0.8467\n",
      "Epoch 122/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.8278 - val_loss: 0.5585 - val_accuracy: 0.8500\n",
      "Epoch 123/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.8239 - val_loss: 0.5698 - val_accuracy: 0.8417\n",
      "Epoch 124/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.8256 - val_loss: 0.5804 - val_accuracy: 0.8583\n",
      "Epoch 125/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.8259 - val_loss: 0.5510 - val_accuracy: 0.8533\n",
      "Epoch 126/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5442 - accuracy: 0.8239 - val_loss: 0.5630 - val_accuracy: 0.8533\n",
      "Epoch 127/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.8326 - val_loss: 0.5558 - val_accuracy: 0.8367\n",
      "Epoch 128/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.8294 - val_loss: 0.5602 - val_accuracy: 0.8600\n",
      "Epoch 129/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.8265 - val_loss: 0.5725 - val_accuracy: 0.8450\n",
      "Epoch 130/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5439 - accuracy: 0.8254 - val_loss: 0.5632 - val_accuracy: 0.8567\n",
      "Epoch 131/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.8309 - val_loss: 0.5510 - val_accuracy: 0.8550\n",
      "Epoch 132/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.8241 - val_loss: 0.5561 - val_accuracy: 0.8583\n",
      "Epoch 133/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5496 - accuracy: 0.8274 - val_loss: 0.5472 - val_accuracy: 0.8517\n",
      "Epoch 134/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5262 - accuracy: 0.8294 - val_loss: 0.5387 - val_accuracy: 0.8617\n",
      "Epoch 135/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.8367 - val_loss: 0.5347 - val_accuracy: 0.8600\n",
      "Epoch 136/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5201 - accuracy: 0.8361 - val_loss: 0.5636 - val_accuracy: 0.8600\n",
      "Epoch 137/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5406 - accuracy: 0.8293 - val_loss: 0.5507 - val_accuracy: 0.8500\n",
      "Epoch 138/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.8337 - val_loss: 0.5666 - val_accuracy: 0.8667\n",
      "Epoch 139/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.8317 - val_loss: 0.5388 - val_accuracy: 0.8567\n",
      "Epoch 140/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.8369 - val_loss: 0.5601 - val_accuracy: 0.8600\n",
      "Epoch 141/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.8287 - val_loss: 0.5775 - val_accuracy: 0.8517\n",
      "Epoch 142/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.8270 - val_loss: 0.5333 - val_accuracy: 0.8500\n",
      "Epoch 143/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.8352 - val_loss: 0.5638 - val_accuracy: 0.8433\n",
      "Epoch 144/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.8328 - val_loss: 0.5682 - val_accuracy: 0.8633\n",
      "Epoch 145/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.8363 - val_loss: 0.5541 - val_accuracy: 0.8633\n",
      "Epoch 146/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.8281 - val_loss: 0.5417 - val_accuracy: 0.8517\n",
      "Epoch 147/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.8326 - val_loss: 0.5684 - val_accuracy: 0.8467\n",
      "Epoch 148/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5313 - accuracy: 0.8298 - val_loss: 0.5372 - val_accuracy: 0.8500\n",
      "Epoch 149/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.8341 - val_loss: 0.5537 - val_accuracy: 0.8700\n",
      "Epoch 150/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.8404 - val_loss: 0.5596 - val_accuracy: 0.8633\n",
      "Epoch 151/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.8417 - val_loss: 0.5209 - val_accuracy: 0.8667\n",
      "Epoch 152/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.8324 - val_loss: 0.5455 - val_accuracy: 0.8583\n",
      "Epoch 153/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.8391 - val_loss: 0.5410 - val_accuracy: 0.8683\n",
      "Epoch 154/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5252 - accuracy: 0.8300 - val_loss: 0.5355 - val_accuracy: 0.8600\n",
      "Epoch 155/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.8400 - val_loss: 0.5459 - val_accuracy: 0.8533\n",
      "Epoch 156/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.8402 - val_loss: 0.5522 - val_accuracy: 0.8500\n",
      "Epoch 157/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.8426 - val_loss: 0.5485 - val_accuracy: 0.8533\n",
      "Epoch 158/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.8396 - val_loss: 0.5480 - val_accuracy: 0.8600\n",
      "Epoch 159/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.8381 - val_loss: 0.5438 - val_accuracy: 0.8633\n",
      "Epoch 160/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5078 - accuracy: 0.8359 - val_loss: 0.5481 - val_accuracy: 0.8583\n",
      "Epoch 161/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.8428 - val_loss: 0.5541 - val_accuracy: 0.8650\n",
      "Epoch 162/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.8430 - val_loss: 0.5411 - val_accuracy: 0.8567\n",
      "Epoch 163/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.8430 - val_loss: 0.5735 - val_accuracy: 0.8650\n",
      "Epoch 164/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.8383 - val_loss: 0.5410 - val_accuracy: 0.8733\n",
      "Epoch 165/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5202 - accuracy: 0.8322 - val_loss: 0.5906 - val_accuracy: 0.8600\n",
      "Epoch 166/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.8439 - val_loss: 0.5595 - val_accuracy: 0.8533\n",
      "Epoch 167/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.8502 - val_loss: 0.5504 - val_accuracy: 0.8633\n",
      "Epoch 168/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.8326 - val_loss: 0.5560 - val_accuracy: 0.8300\n",
      "Epoch 169/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.8444 - val_loss: 0.5338 - val_accuracy: 0.8583\n",
      "Epoch 170/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4962 - accuracy: 0.8409 - val_loss: 0.5235 - val_accuracy: 0.8533\n",
      "Epoch 171/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.8398 - val_loss: 0.5449 - val_accuracy: 0.8717\n",
      "Epoch 172/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.8444 - val_loss: 0.5294 - val_accuracy: 0.8533\n",
      "Epoch 173/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4963 - accuracy: 0.8485 - val_loss: 0.5357 - val_accuracy: 0.8517\n",
      "Epoch 174/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4983 - accuracy: 0.8409 - val_loss: 0.5196 - val_accuracy: 0.8600\n",
      "Epoch 175/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4998 - accuracy: 0.8441 - val_loss: 0.5263 - val_accuracy: 0.8833\n",
      "Epoch 176/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.8456 - val_loss: 0.5257 - val_accuracy: 0.8517\n",
      "Epoch 177/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.8463 - val_loss: 0.5516 - val_accuracy: 0.8267\n",
      "Epoch 178/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.8387 - val_loss: 0.5256 - val_accuracy: 0.8717\n",
      "Epoch 179/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.8406 - val_loss: 0.5343 - val_accuracy: 0.8567\n",
      "Epoch 180/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4770 - accuracy: 0.8476 - val_loss: 0.5625 - val_accuracy: 0.8633\n",
      "Epoch 181/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4946 - accuracy: 0.8433 - val_loss: 0.5165 - val_accuracy: 0.8517\n",
      "Epoch 182/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.8454 - val_loss: 0.5217 - val_accuracy: 0.8683\n",
      "Epoch 183/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.8472 - val_loss: 0.5434 - val_accuracy: 0.8567\n",
      "Epoch 184/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.8396 - val_loss: 0.5337 - val_accuracy: 0.8367\n",
      "Epoch 185/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.8470 - val_loss: 0.5334 - val_accuracy: 0.8633\n",
      "Epoch 186/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.8504 - val_loss: 0.5385 - val_accuracy: 0.8583\n",
      "Epoch 187/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.8467 - val_loss: 0.5214 - val_accuracy: 0.8617\n",
      "Epoch 188/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.8470 - val_loss: 0.5247 - val_accuracy: 0.8617\n",
      "Epoch 189/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.8526 - val_loss: 0.5375 - val_accuracy: 0.8650\n",
      "Epoch 190/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.8459 - val_loss: 0.5393 - val_accuracy: 0.8650\n",
      "Epoch 191/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.8480 - val_loss: 0.5314 - val_accuracy: 0.8683\n",
      "Epoch 192/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.8456 - val_loss: 0.5271 - val_accuracy: 0.8700\n",
      "Epoch 193/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.8498 - val_loss: 0.5253 - val_accuracy: 0.8533\n",
      "Epoch 194/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.8454 - val_loss: 0.5241 - val_accuracy: 0.8633\n",
      "Epoch 195/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.8489 - val_loss: 0.5146 - val_accuracy: 0.8750\n",
      "Epoch 196/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.8535 - val_loss: 0.5440 - val_accuracy: 0.8567\n",
      "Epoch 197/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.8459 - val_loss: 0.5131 - val_accuracy: 0.8633\n",
      "Epoch 198/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8489 - val_loss: 0.5153 - val_accuracy: 0.8750\n",
      "Epoch 199/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.8526 - val_loss: 0.5381 - val_accuracy: 0.8600\n",
      "Epoch 200/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.8509 - val_loss: 0.5339 - val_accuracy: 0.8567\n",
      "Epoch 201/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.8511 - val_loss: 0.5284 - val_accuracy: 0.8600\n",
      "Epoch 202/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.8483 - val_loss: 0.4939 - val_accuracy: 0.8717\n",
      "Epoch 203/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4813 - accuracy: 0.8463 - val_loss: 0.5363 - val_accuracy: 0.8533\n",
      "Epoch 204/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4863 - accuracy: 0.8515 - val_loss: 0.5094 - val_accuracy: 0.8717\n",
      "Epoch 205/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4799 - accuracy: 0.8461 - val_loss: 0.5019 - val_accuracy: 0.8750\n",
      "Epoch 206/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4755 - accuracy: 0.8565 - val_loss: 0.5196 - val_accuracy: 0.8617\n",
      "Epoch 207/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.8491 - val_loss: 0.5013 - val_accuracy: 0.8717\n",
      "Epoch 208/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.8539 - val_loss: 0.5268 - val_accuracy: 0.8517\n",
      "Epoch 209/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4761 - accuracy: 0.8494 - val_loss: 0.5562 - val_accuracy: 0.8467\n",
      "Epoch 210/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4939 - accuracy: 0.8528 - val_loss: 0.5242 - val_accuracy: 0.8633\n",
      "Epoch 211/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.8537 - val_loss: 0.5189 - val_accuracy: 0.8617\n",
      "Epoch 212/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.8598 - val_loss: 0.5324 - val_accuracy: 0.8567\n",
      "Epoch 213/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4763 - accuracy: 0.8559 - val_loss: 0.4921 - val_accuracy: 0.8683\n",
      "Epoch 214/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.8535 - val_loss: 0.5139 - val_accuracy: 0.8700\n",
      "Epoch 215/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.8500 - val_loss: 0.4900 - val_accuracy: 0.8700\n",
      "Epoch 216/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.8543 - val_loss: 0.4964 - val_accuracy: 0.8717\n",
      "Epoch 217/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.8535 - val_loss: 0.5085 - val_accuracy: 0.8750\n",
      "Epoch 218/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.8585 - val_loss: 0.5007 - val_accuracy: 0.8667\n",
      "Epoch 219/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.8576 - val_loss: 0.4937 - val_accuracy: 0.8750\n",
      "Epoch 220/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.8493 - val_loss: 0.5186 - val_accuracy: 0.8617\n",
      "Epoch 221/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4707 - accuracy: 0.8548 - val_loss: 0.5350 - val_accuracy: 0.8650\n",
      "Epoch 222/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.8517 - val_loss: 0.4955 - val_accuracy: 0.8533\n",
      "Epoch 223/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.8530 - val_loss: 0.5026 - val_accuracy: 0.8733\n",
      "Epoch 224/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4736 - accuracy: 0.8543 - val_loss: 0.5201 - val_accuracy: 0.8650\n",
      "Epoch 225/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.8587 - val_loss: 0.4826 - val_accuracy: 0.8733\n",
      "Epoch 226/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.8578 - val_loss: 0.4993 - val_accuracy: 0.8700\n",
      "Epoch 227/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4680 - accuracy: 0.8594 - val_loss: 0.4975 - val_accuracy: 0.8667\n",
      "Epoch 228/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.8611 - val_loss: 0.5227 - val_accuracy: 0.8550\n",
      "Epoch 229/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.8567 - val_loss: 0.5329 - val_accuracy: 0.8683\n",
      "Epoch 230/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.8548 - val_loss: 0.4978 - val_accuracy: 0.8600\n",
      "Epoch 231/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.8554 - val_loss: 0.5124 - val_accuracy: 0.8733\n",
      "Epoch 232/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.8544 - val_loss: 0.5054 - val_accuracy: 0.8667\n",
      "Epoch 233/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.8596 - val_loss: 0.5360 - val_accuracy: 0.8767\n",
      "Epoch 234/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.8563 - val_loss: 0.5081 - val_accuracy: 0.8733\n",
      "Epoch 235/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.8554 - val_loss: 0.5501 - val_accuracy: 0.8633\n",
      "Epoch 236/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4715 - accuracy: 0.8572 - val_loss: 0.5081 - val_accuracy: 0.8717\n",
      "Epoch 237/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.8537 - val_loss: 0.4954 - val_accuracy: 0.8783\n",
      "Epoch 238/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.8661 - val_loss: 0.4985 - val_accuracy: 0.8867\n",
      "Epoch 239/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.8567 - val_loss: 0.4876 - val_accuracy: 0.8733\n",
      "Epoch 240/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.8606 - val_loss: 0.5044 - val_accuracy: 0.8717\n",
      "Epoch 241/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4683 - accuracy: 0.8581 - val_loss: 0.5043 - val_accuracy: 0.8633\n",
      "Epoch 242/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.8628 - val_loss: 0.4875 - val_accuracy: 0.8783\n",
      "Epoch 243/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.8574 - val_loss: 0.5011 - val_accuracy: 0.8733\n",
      "Epoch 244/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.8587 - val_loss: 0.5130 - val_accuracy: 0.8883\n",
      "Epoch 245/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.8594 - val_loss: 0.5072 - val_accuracy: 0.8700\n",
      "Epoch 246/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.8600 - val_loss: 0.5045 - val_accuracy: 0.8717\n",
      "Epoch 247/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.8561 - val_loss: 0.4958 - val_accuracy: 0.8867\n",
      "Epoch 248/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.8537 - val_loss: 0.5256 - val_accuracy: 0.8667\n",
      "Epoch 249/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.8520 - val_loss: 0.5147 - val_accuracy: 0.8583\n",
      "Epoch 250/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.8598 - val_loss: 0.5201 - val_accuracy: 0.8667\n",
      "Epoch 251/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4585 - accuracy: 0.8619 - val_loss: 0.5249 - val_accuracy: 0.8650\n",
      "Epoch 252/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.8630 - val_loss: 0.5473 - val_accuracy: 0.8733\n",
      "Epoch 253/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.8581 - val_loss: 0.5036 - val_accuracy: 0.8733\n",
      "Epoch 254/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.8602 - val_loss: 0.4839 - val_accuracy: 0.8750\n",
      "Epoch 255/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.8609 - val_loss: 0.5026 - val_accuracy: 0.8683\n",
      "Epoch 256/300\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.8576 - val_loss: 0.5038 - val_accuracy: 0.8567\n",
      "Epoch 257/300\n",
      "151/169 [=========================>....] - ETA: 0s - loss: 0.4437 - accuracy: 0.8651"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 26\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/keras/engine/training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1605\u001b[0m     )\n\u001b[0;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1621\u001b[0m }\n\u001b[1;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/keras/engine/training.py:1939\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test_counter\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1938\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_test_begin()\n\u001b[0;32m-> 1939\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/keras/engine/data_adapter.py:1307\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1307\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 696\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[1;32m    717\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    718\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v3(\n\u001b[1;32m    719\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[1;32m    720\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[0;32m--> 721\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/3222/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3409\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3408\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3409\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3410\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3412\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1],df_train.iloc[:,-1], test_size=0.1, random_state=10)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=128, activation='elu', input_shape=(128,)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=256, activation='elu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=128, activation='elu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1],df_train.iloc[:,-1], test_size=0.1, random_state=None)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=128, activation='elu', input_shape=(128,)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=512, activation='elu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(units=512, activation='elu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=128, activation='elu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05d0a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Search\n",
      "Best Parameters:  {'max_depth': None, 'max_features': 'log2', 'n_estimators': 300}\n",
      "Best Score:  0.9502083333333333\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Evaluate the best model on the test set\u001b[39;00m\n\u001b[1;32m     24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m(y_test, y_pred))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1],df_train.iloc[:,-1], test_size=0.2, random_state=10)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees\n",
    "    'max_depth': [None, 5, 10],  # Maximum depth of each tree\n",
    "    'max_features': ['sqrt', 'log2']  # Number of features to consider for the best split\n",
    "}\n",
    "\n",
    "# Create an instance of the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting Search\")\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b34af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'learning_rate': 0.5, 'n_estimators': 300}\n",
      "Best Score:  0.8791666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1],df_train.iloc[:,-1], test_size=0.2, random_state=10)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200,300,400],  # Number of weak learners (decision trees)\n",
    "    'learning_rate': [0.1,0.2,0.3, 0.5, 1.0]  # Learning rate\n",
    "}\n",
    "\n",
    "# Create an instance of the AdaBoost classifier\n",
    "adaboost_classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(adaboost_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldForest(num):\n",
    "    \n",
    "    RF_accuracy =[]\n",
    "    kf = KFold(n_splits=num,shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(df_train)):\n",
    "        print(f\"Fold {i}:\")\n",
    "#         print(f\"  Train: index={train_index}\") \n",
    "#         print(f\"  Test:  index={test_index}\")\n",
    "        X_train=df_train.iloc[train_index,:-1]\n",
    "        X_test=df_train.iloc[test_index,:-1]\n",
    "        y_train=df_train.iloc[train_index,-1]\n",
    "        y_test=df_train.iloc[test_index,-1]\n",
    "        \n",
    "        \n",
    "        pipe = DecisionTreeClassifier()\n",
    "        pipe.fit(X_train, y_train)\n",
    "        RF_accuracy.append(pipe.score(X_test, y_test)) \n",
    "        \n",
    "    \n",
    "    \n",
    "    x_axis = [\"Fold \"+ str(i+1) for i in range(len(RF_accuracy))]\n",
    "    y_axis = RF_accuracy\n",
    "    plt.bar(x_axis, y_axis)\n",
    "    plt.title('Random Forest Results with K-fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    for i, v in enumerate(y_axis):\n",
    "        plt.text(i-0.25, v+0.01, \"{:.3f}\".format(v))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad718e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
